#!/usr/bin/env python3
"""
BridgeFacile - Traitement des PDFs du Code 2017
===============================================

Module spÃ©cialisÃ© pour parser les PDFs du Code 2017 de bridge,
extraire les articles de loi et dÃ©tecter les rÃ©fÃ©rences entre articles.

Auteur: BridgeFacile Team
Date: 2025-01-07
"""

import os
import sys
import re
import csv
import json
import logging
from typing import Dict, List, Tuple, Set, Optional, Any
from datetime import datetime
import shutil

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("bridge_law_parser.log")
    ]
)
logger = logging.getLogger(__name__)

class BridgeLawParser:
    """
    Parser spÃ©cialisÃ© pour les documents du Code 2017 de bridge.
    Extrait les articles de loi et dÃ©tecte les rÃ©fÃ©rences entre articles.
    """
    
    def __init__(self, 
                 input_dir: str = "./input",
                 output_dir: str = "./output"):
        """
        Initialise le parser avec les rÃ©pertoires d'entrÃ©e et de sortie.
        
        Args:
            input_dir: RÃ©pertoire contenant les PDFs du Code 2017
            output_dir: RÃ©pertoire de sortie pour les fichiers CSV
        """
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.available_methods = self._detect_available_methods()
        
        # CrÃ©er le rÃ©pertoire de sortie s'il n'existe pas
        os.makedirs(output_dir, exist_ok=True)
        
        # Patterns regex pour la dÃ©tection des articles et rÃ©fÃ©rences
        self.article_pattern = re.compile(r'(?:LOI|ARTICLE)\s+(\d+[A-Z]?(?:\.\d+)?)', re.IGNORECASE)
        self.reference_pattern = re.compile(r'(?:Loi|Article|Voir|Cf\.?)\s+(\d+[A-Z]?(?:\.\d+)?)', re.IGNORECASE)
        
        # Stockage des articles et rÃ©fÃ©rences
        self.articles = {}  # {article_id: {content, title, references, ...}}
        self.references = {}  # {article_id: [referenced_articles]}
        
        logger.info(f"Parser initialisÃ© - EntrÃ©e: {input_dir}, Sortie: {output_dir}")
        logger.info(f"MÃ©thodes disponibles: {', '.join(self.available_methods)}")
    
    def _detect_available_methods(self) -> List[str]:
        """DÃ©tecte les mÃ©thodes de parsing disponibles."""
        methods = []
        
        try:
            import pdfplumber
            methods.append('pdfplumber')
        except ImportError:
            pass
        
        try:
            import PyPDF2
            methods.append('pypdf2')
        except ImportError:
            pass
        
        if os.system('which pdftotext > /dev/null 2>&1') == 0:
            methods.append('pdftotext')
        
        return methods
    
    def parse_pdf(self, pdf_path: str, method: Optional[str] = None) -> Dict[str, Any]:
        """Parse un PDF et retourne le contenu extrait."""
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"Fichier PDF non trouvÃ©: {pdf_path}")
        
        if method is None:
            if 'pdfplumber' in self.available_methods:
                method = 'pdfplumber'
            elif 'pypdf2' in self.available_methods:
                method = 'pypdf2'
            elif 'pdftotext' in self.available_methods:
                method = 'pdftotext'
            else:
                raise RuntimeError("Aucune mÃ©thode de parsing disponible. Installez pdfplumber ou PyPDF2.")
        
        logger.info(f"Parsing de {pdf_path} avec la mÃ©thode: {method}")
        
        if method == 'pdfplumber':
            return self._parse_with_pdfplumber(pdf_path)
        elif method == 'pypdf2':
            return self._parse_with_pypdf2(pdf_path)
        elif method == 'pdftotext':
            return self._parse_with_pdftotext(pdf_path)
        else:
            raise ValueError(f"MÃ©thode de parsing non supportÃ©e: {method}")
    
    def _parse_with_pdfplumber(self, pdf_path: str) -> Dict[str, Any]:
        """Parse avec pdfplumber (mÃ©thode recommandÃ©e)."""
        import pdfplumber
        
        result = {
            'text': '',
            'pages': [],
            'metadata': {},
            'method': 'pdfplumber'
        }
        
        with pdfplumber.open(pdf_path) as pdf:
            result['metadata'] = {
                'num_pages': len(pdf.pages),
                'file_size': os.path.getsize(pdf_path),
                'filename': os.path.basename(pdf_path)
            }
            
            for i, page in enumerate(pdf.pages):
                page_text = page.extract_text() or ''
                result['pages'].append({
                    'page_number': i + 1,
                    'text': page_text,
                    'char_count': len(page_text)
                })
                result['text'] += page_text + '\n'
        
        return result
    
    def _parse_with_pypdf2(self, pdf_path: str) -> Dict[str, Any]:
        """Parse avec PyPDF2 (mÃ©thode de fallback)."""
        import PyPDF2
        
        result = {
            'text': '',
            'pages': [],
            'metadata': {},
            'method': 'pypdf2'
        }
        
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            
            result['metadata'] = {
                'num_pages': len(reader.pages),
                'file_size': os.path.getsize(pdf_path),
                'filename': os.path.basename(pdf_path)
            }
            
            for i, page in enumerate(reader.pages):
                page_text = page.extract_text() or ''
                result['pages'].append({
                    'page_number': i + 1,
                    'text': page_text,
                    'char_count': len(page_text)
                })
                result['text'] += page_text + '\n'
        
        return result
    
    def _parse_with_pdftotext(self, pdf_path: str) -> Dict[str, Any]:
        """Parse avec pdftotext (mÃ©thode systÃ¨me)."""
        import subprocess
        
        result = {
            'text': '',
            'pages': [],
            'metadata': {},
            'method': 'pdftotext'
        }
        
        try:
            output = subprocess.run(
                ['pdftotext', pdf_path, '-'],
                capture_output=True,
                text=True,
                check=True
            )
            
            result['text'] = output.stdout
            result['metadata'] = {
                'file_size': os.path.getsize(pdf_path),
                'filename': os.path.basename(pdf_path)
            }
            
            # Estimation du nombre de pages (approximative)
            page_breaks = result['text'].count('\f')
            result['metadata']['num_pages'] = max(1, page_breaks + 1)
            
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Erreur pdftotext: {e}")
        
        return result
    
    def extract_articles(self, pdf_path: str) -> Dict[str, Dict[str, Any]]:
        """
        Extrait les articles de loi d'un PDF du Code 2017.
        
        Returns:
            Dict avec les articles extraits {article_id: {content, title, ...}}
        """
        # Parser le PDF
        result = self.parse_pdf(pdf_path)
        text = result['text']
        
        # Extraire les articles
        articles = {}
        current_article = None
        current_content = []
        
        # Diviser le texte en lignes
        lines = text.split('\n')
        
        for line in lines:
            # DÃ©tecter un nouvel article
            article_match = self.article_pattern.search(line)
            
            if article_match:
                # Si on Ã©tait dÃ©jÃ  en train de traiter un article, le sauvegarder
                if current_article:
                    articles[current_article] = {
                        'content': '\n'.join(current_content),
                        'title': current_content[0] if current_content else '',
                        'references': self._extract_references('\n'.join(current_content)),
                        'source_file': os.path.basename(pdf_path)
                    }
                
                # Commencer un nouvel article
                current_article = article_match.group(1)
                current_content = [line]
            elif current_article:
                # Continuer Ã  ajouter du contenu Ã  l'article courant
                current_content.append(line)
        
        # Sauvegarder le dernier article
        if current_article and current_content:
            articles[current_article] = {
                'content': '\n'.join(current_content),
                'title': current_content[0] if current_content else '',
                'references': self._extract_references('\n'.join(current_content)),
                'source_file': os.path.basename(pdf_path)
            }
        
        logger.info(f"Extraction terminÃ©e: {len(articles)} articles trouvÃ©s dans {pdf_path}")
        return articles
    
    def _extract_references(self, text: str) -> List[str]:
        """
        Extrait les rÃ©fÃ©rences vers d'autres articles dans un texte.
        
        Returns:
            Liste des IDs d'articles rÃ©fÃ©rencÃ©s
        """
        references = []
        
        # Trouver toutes les rÃ©fÃ©rences
        for match in self.reference_pattern.finditer(text):
            ref = match.group(1)
            if ref not in references:
                references.append(ref)
        
        return references
    
    def process_all_pdfs(self) -> Dict[str, Dict[str, Any]]:
        """
        Traite tous les PDFs du rÃ©pertoire d'entrÃ©e.
        
        Returns:
            Dict avec tous les articles extraits
        """
        all_articles = {}
        
        # Lister tous les fichiers PDF
        pdf_files = [f for f in os.listdir(self.input_dir) if f.lower().endswith('.pdf')]
        
        if not pdf_files:
            logger.warning(f"Aucun fichier PDF trouvÃ© dans {self.input_dir}")
            return all_articles
        
        logger.info(f"Traitement de {len(pdf_files)} fichiers PDF...")
        
        for pdf_file in pdf_files:
            pdf_path = os.path.join(self.input_dir, pdf_file)
            try:
                # Extraire les articles
                articles = self.extract_articles(pdf_path)
                
                # Ajouter Ã  la collection complÃ¨te
                all_articles.update(articles)
                
                logger.info(f"âœ… {pdf_file}: {len(articles)} articles extraits")
            except Exception as e:
                logger.error(f"âŒ Erreur avec {pdf_file}: {e}")
        
        # Mettre Ã  jour les rÃ©fÃ©rences croisÃ©es
        self._update_cross_references(all_articles)
        
        # Sauvegarder les articles
        self.articles = all_articles
        
        return all_articles
    
    def _update_cross_references(self, articles: Dict[str, Dict[str, Any]]) -> None:
        """
        Met Ã  jour les rÃ©fÃ©rences croisÃ©es entre articles.
        
        Args:
            articles: Dict avec tous les articles extraits
        """
        # Construire le graphe de rÃ©fÃ©rences
        references = {}
        
        for article_id, article_data in articles.items():
            refs = article_data.get('references', [])
            references[article_id] = refs
            
            # Ajouter les rÃ©fÃ©rences inverses
            for ref in refs:
                if ref in articles:
                    if 'referenced_by' not in articles[ref]:
                        articles[ref]['referenced_by'] = []
                    
                    if article_id not in articles[ref]['referenced_by']:
                        articles[ref]['referenced_by'].append(article_id)
        
        # Sauvegarder les rÃ©fÃ©rences
        self.references = references
        
        logger.info(f"RÃ©fÃ©rences croisÃ©es mises Ã  jour pour {len(articles)} articles")
    
    def save_to_csv(self) -> Dict[str, str]:
        """
        Sauvegarde les articles extraits en CSV.
        
        Returns:
            Dict avec les chemins des fichiers CSV crÃ©Ã©s
        """
        if not self.articles:
            logger.warning("Aucun article Ã  sauvegarder. ExÃ©cutez process_all_pdfs() d'abord.")
            return {}
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_files = {}
        
        # 1. CSV des articles
        articles_csv = os.path.join(self.output_dir, f"bridge_articles_{timestamp}.csv")
        with open(articles_csv, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([
                'Article_ID', 'Titre', 'Contenu', 'Source', 'RÃ©fÃ©rences', 'RÃ©fÃ©rencÃ©_Par'
            ])
            
            for article_id, article_data in self.articles.items():
                writer.writerow([
                    article_id,
                    article_data.get('title', ''),
                    article_data.get('content', ''),
                    article_data.get('source_file', ''),
                    ','.join(article_data.get('references', [])),
                    ','.join(article_data.get('referenced_by', []))
                ])
        
        csv_files['articles'] = articles_csv
        logger.info(f"Articles sauvegardÃ©s dans: {articles_csv}")
        
        # 2. CSV des rÃ©fÃ©rences
        references_csv = os.path.join(self.output_dir, f"bridge_references_{timestamp}.csv")
        with open(references_csv, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['Article_Source', 'Article_Cible'])
            
            for article_id, refs in self.references.items():
                for ref in refs:
                    if ref in self.articles:  # Ne sauvegarder que les rÃ©fÃ©rences valides
                        writer.writerow([article_id, ref])
        
        csv_files['references'] = references_csv
        logger.info(f"RÃ©fÃ©rences sauvegardÃ©es dans: {references_csv}")
        
        # 3. CSV des mÃ©tadonnÃ©es
        metadata_csv = os.path.join(self.output_dir, f"bridge_metadata_{timestamp}.csv")
        with open(metadata_csv, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([
                'Article_ID', 'Nombre_Mots', 'Nombre_RÃ©fÃ©rences', 'Nombre_Citations'
            ])
            
            for article_id, article_data in self.articles.items():
                content = article_data.get('content', '')
                word_count = len(content.split())
                ref_count = len(article_data.get('references', []))
                cited_count = len(article_data.get('referenced_by', []))
                
                writer.writerow([
                    article_id,
                    word_count,
                    ref_count,
                    cited_count
                ])
        
        csv_files['metadata'] = metadata_csv
        logger.info(f"MÃ©tadonnÃ©es sauvegardÃ©es dans: {metadata_csv}")
        
        # 4. Fichier JSON complet (pour intÃ©gration web)
        json_file = os.path.join(self.output_dir, f"bridge_law_data_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.articles, f, ensure_ascii=False, indent=2)
        
        csv_files['json'] = json_file
        logger.info(f"DonnÃ©es JSON sauvegardÃ©es dans: {json_file}")
        
        return csv_files
    
    def create_individual_article_files(self) -> List[str]:
        """
        CrÃ©e des fichiers individuels pour chaque article (pour l'intÃ©gration web).
        
        Returns:
            Liste des chemins des fichiers crÃ©Ã©s
        """
        if not self.articles:
            logger.warning("Aucun article Ã  sauvegarder. ExÃ©cutez process_all_pdfs() d'abord.")
            return []
        
        # CrÃ©er un rÃ©pertoire pour les articles individuels
        articles_dir = os.path.join(self.output_dir, "articles")
        os.makedirs(articles_dir, exist_ok=True)
        
        created_files = []
        
        for article_id, article_data in self.articles.items():
            # Nettoyer l'ID pour le nom de fichier
            safe_id = article_id.replace('.', '_')
            file_path = os.path.join(articles_dir, f"article_{safe_id}.json")
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2)
            
            created_files.append(file_path)
        
        logger.info(f"{len(created_files)} fichiers d'articles individuels crÃ©Ã©s dans: {articles_dir}")
        return created_files
    
    def generate_navigation_index(self) -> str:
        """
        GÃ©nÃ¨re un fichier d'index pour la navigation entre articles.
        
        Returns:
            Chemin du fichier d'index crÃ©Ã©
        """
        if not self.articles:
            logger.warning("Aucun article Ã  indexer. ExÃ©cutez process_all_pdfs() d'abord.")
            return ""
        
        # CrÃ©er l'index
        index = {
            "articles": {},
            "categories": {},
            "stats": {
                "total_articles": len(self.articles),
                "total_references": sum(len(refs) for refs in self.references.values())
            }
        }
        
        # Ajouter les articles Ã  l'index
        for article_id, article_data in self.articles.items():
            # Version simplifiÃ©e pour l'index
            index["articles"][article_id] = {
                "title": article_data.get('title', ''),
                "references": article_data.get('references', []),
                "referenced_by": article_data.get('referenced_by', []),
                "source_file": article_data.get('source_file', '')
            }
        
        # Sauvegarder l'index
        index_file = os.path.join(self.output_dir, "bridge_law_index.json")
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Index de navigation crÃ©Ã©: {index_file}")
        return index_file
    
    def generate_web_assets(self) -> Dict[str, str]:
        """
        GÃ©nÃ¨re les fichiers nÃ©cessaires pour l'intÃ©gration web.
        
        Returns:
            Dict avec les chemins des fichiers crÃ©Ã©s
        """
        web_assets = {}
        
        # 1. CrÃ©er le rÃ©pertoire des assets web
        web_dir = os.path.join(self.output_dir, "web")
        os.makedirs(web_dir, exist_ok=True)
        
        # 2. Copier les donnÃ©es JSON
        json_file = os.path.join(self.output_dir, "bridge_law_data.json")
        if os.path.exists(json_file):
            web_json = os.path.join(web_dir, "bridge_law_data.json")
            shutil.copy(json_file, web_json)
            web_assets['json'] = web_json
        
        # 3. CrÃ©er un fichier de configuration
        config = {
            "version": "1.0.0",
            "timestamp": datetime.now().isoformat(),
            "article_count": len(self.articles),
            "reference_count": sum(len(refs) for refs in self.references.values()),
            "structure": {
                "articles": "Liste des articles avec leur contenu",
                "references": "Graphe des rÃ©fÃ©rences entre articles",
                "metadata": "MÃ©tadonnÃ©es sur les articles"
            }
        }
        
        config_file = os.path.join(web_dir, "config.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        web_assets['config'] = config_file
        
        logger.info(f"Assets web gÃ©nÃ©rÃ©s dans: {web_dir}")
        return web_assets
    
    def run_full_process(self) -> Dict[str, Any]:
        """
        ExÃ©cute le processus complet de traitement des PDFs.
        
        Returns:
            Dict avec les rÃ©sultats du traitement
        """
        logger.info("DÃ©marrage du traitement complet...")
        
        # 1. Traiter tous les PDFs
        self.process_all_pdfs()
        
        # 2. Sauvegarder en CSV
        csv_files = self.save_to_csv()
        
        # 3. CrÃ©er les fichiers individuels
        article_files = self.create_individual_article_files()
        
        # 4. GÃ©nÃ©rer l'index de navigation
        index_file = self.generate_navigation_index()
        
        # 5. GÃ©nÃ©rer les assets web
        web_assets = self.generate_web_assets()
        
        results = {
            "articles": len(self.articles),
            "references": sum(len(refs) for refs in self.references.values()),
            "csv_files": csv_files,
            "article_files": len(article_files),
            "index_file": index_file,
            "web_assets": web_assets
        }
        
        logger.info(f"Traitement complet terminÃ©: {len(self.articles)} articles extraits")
        return results


def install_dependencies():
    """Installe les dÃ©pendances nÃ©cessaires."""
    import subprocess
    import sys
    
    packages = ['pdfplumber', 'PyPDF2']
    
    print("Installation des dÃ©pendances...")
    for package in packages:
        try:
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])
            print(f"âœ… {package} installÃ©")
        except subprocess.CalledProcessError:
            print(f"âŒ Erreur installation {package}")


if __name__ == "__main__":
    # VÃ©rifier les arguments
    if len(sys.argv) > 1 and sys.argv[1] == "--install":
        install_dependencies()
        sys.exit(0)
    
    # RÃ©pertoires par dÃ©faut
    input_dir = "./input"
    output_dir = "./output"
    
    # Permettre de spÃ©cifier les rÃ©pertoires en ligne de commande
    if len(sys.argv) > 2:
        input_dir = sys.argv[1]
        output_dir = sys.argv[2]
    
    # CrÃ©er le parser
    parser = BridgeLawParser(input_dir, output_dir)
    
    # ExÃ©cuter le traitement complet
    try:
        results = parser.run_full_process()
        
        print("\nâœ… Traitement terminÃ© avec succÃ¨s!")
        print(f"ğŸ“Š Articles extraits: {results['articles']}")
        print(f"ğŸ”— RÃ©fÃ©rences dÃ©tectÃ©es: {results['references']}")
        print("\nğŸ“ Fichiers gÃ©nÃ©rÃ©s:")
        
        for csv_type, csv_path in results['csv_files'].items():
            print(f"  - {csv_type}: {os.path.basename(csv_path)}")
        
        print(f"\nğŸ“‚ RÃ©pertoire de sortie: {output_dir}")
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        logger.error(f"Erreur lors du traitement: {e}", exc_info=True)

